To evaluate the effectiveness of different AI agents in generating lecture material, we followed a structured methodology consisting of four stages:

## 3.1 Choosing the Topic
We selected cache memories as our focus topic because it is one of the most complex topics in our Computer Architecture course, which allowed us to challenge the AI agents. The topic includes theoretical concepts and practical applications, so we could test how well the AI explains technical ideas and how accurately it answers practice questions. This made it a strong choice for evaluating different aspects of AI-generated lecture material.
We worked on 6 subtopics:
Memory hierarchy
Associative memory (CAM)
Goal of caching, management and delay
Direct Mapped Cache
Fully Associative Cache
Set Associative Cache

## 3.2 AI Agent Selection


During our initial, more basic searches with various AI agents, we noticed that some of them gave identical analogies, while others produced different ones. After further investigation, we discovered that the AI agents producing similar analogies were often developed in the same geographical area. To avoid bias and promote diversity in AI behavior, we selected one or two models from each of three different regions. From the United States, we decided to use ChatGPT-4o that was developed by OpenAI, which could generate lecture plans as well as images using its image generation model DALL¬∑E 3. Perplexity developed by Perplexity AI has drawn our attention in this project as it not only uses generative AI but also uses web research to integrate in the chatbot experience. Our third model, Le Chat was developed by Mistral which is France based, and our fourth model DeepSeek-V3 by DeepSeek AI was trained using both Chinese and English datasets which we hypothesized to give diverse results.
## 3.3 Developing the Prompt

We took an iterative approach to creating prompts, improving them across three major versions. We created the first one with the help of ChatGPT, hypothesising that it would include all the necessary instructions:
Prompt 1: Minimal Prompting
‚ÄúYou are preparing teaching material for a lecture segment aimed at **second-year computer engineering students**.  
The topic is: **Memory hierarchy as an introduction to cache memories** 
Generate the content using the structure below, to create the result in markdown: 
---
### üìÑ Slide Count
- Prepare **3 to 5 slides**.
- If more are needed for clarity, you may go beyond 5 **only if you explain why**.
---
### üß± For Each Slide, Include:
- **Title**  
  - A clear, concise title that reflects the slide‚Äôs focus.

- **Bullet Points**  
  - 4‚Äì6 bullet points per slide  
  - Each bullet should be a **complete sentence** (not fragments)  
  - Each bullet can be **1‚Äì2 sentences long**  
  - Use precise, technical language but explain new terms simply

- **Speaker Notes**
  - A **paragraph** explaining the slide‚Äôs content in a way a professor might say it aloud  
  - A **brief summary** (1‚Äì2 sentences) of the main idea for presenter use

- **[Optional] Analogy**
  - If helpful for memory or understanding, include **one real-world analogy**

- **[Optional] Diagram Description**
  - If a diagram would aid understanding, describe it in words.  
  - Format:  
    "Diagram: a cache with N sets, each with K lines. One block maps to a set using modulo operation."

---
### üéØ Additional Instructions

- Maintain an **educational and student-friendly tone**
- Assume the student knows digital logic and memory basics, but not advanced architecture
- Avoid unnecessary jargon, or explain it when used
- Focus on helping students build both **intuitive and precise** understanding‚Äù

Gave only the topic name and general instructions. This version revealed how the models interpret open-ended teaching tasks. We noticed frequent scope drift and inconsistent depth.

Prompt 2: Scoped Prompting

Added subtopics, keywords, and specific teaching goals (e.g., ‚ÄúExplain hit/miss in cache using intuitive analogies‚Äù). This version helped narrow the responses and test whether models could respect the scope of the lessons.

Prompt 3: Structured Creative Prompting

Allowed freedom in formatting (tables, diagrams, bullet styles) while keeping scope guidance. This gave the best balance between flexibility and coherence and allowed us to evaluate whether models could provide natural teaching flow.

## 3.4 Collecting Lecture Material
We divided the topic of cache memories as explained in 3.1, and each group member worked on different subtopics and used the same set of prompts with all four AI agents. We improved the prompts as we collected more material. This allowed us to compare the agents under consistent conditions while covering the full lecture scope. 
Each AI agent was asked to generate slides, speaker notes, diagrams (or their descriptions), and follow-up assessment questions. We also examined whether their responses matched our desired scope and whether they could correctly answer each other‚Äôs questions. Additionally, we tested AI objectivity by attempting to persuade each model into accepting incorrect answers to factual questions. 
